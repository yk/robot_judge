{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "# Problem Set 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "Set these to make things run faster. In order to use the maximum of these constants, just set them to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CASES = 1000 # Number of cases (spacys tokenization is quite slow)\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display plots directly inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def counter_message(cnt, max_cnt, action):\n",
    "    \"\"\"Display Progress\n",
    "    \"\"\"   \n",
    "    #Print Message\n",
    "    print(\"\\r{}/{}: {}.\".format(cnt, max_cnt, action), end=\" \"*10, flush=True)\n",
    "\n",
    "    \n",
    "# Load metadata\n",
    "judge_ids = {}\n",
    "is_republican = {}\n",
    "case_reversed = {}\n",
    "\n",
    "metadata = pd.read_csv(\"data/case_metadata.csv\")\n",
    "metadata = metadata.dropna(axis='index', how='any') # remove rows with missing values\n",
    "\n",
    "for caseid, case_rev, judge_id, year, x_republican, log_cites in metadata.values:\n",
    "    judge_ids[caseid] = int(judge_id)\n",
    "    is_republican[caseid] = x_republican\n",
    "    case_reversed[caseid] = case_rev\n",
    "\n",
    "    \n",
    "# Load case data\n",
    "zfile = ZipFile(\"data/cases.zip\")\n",
    "\n",
    "caseids = []\n",
    "raw_texts = {}\n",
    "years = {}\n",
    "\n",
    "\n",
    "# Randomly shuffle files and choose as many as we need\n",
    "files = zfile.namelist()\n",
    "files = np.random.permutation(files)\n",
    "\n",
    "if NUM_CASES is None:\n",
    "    NUM_CASES = len(files)\n",
    "NUM_CASES = min(NUM_CASES, len(metadata))\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for case in files:\n",
    "    if cnt >= NUM_CASES:\n",
    "        break\n",
    "    year, caseid = case[:-4].split(\"_\")\n",
    "    if caseid in metadata['caseid'].values:\n",
    "        cnt += 1\n",
    "        with zfile.open(case) as f:\n",
    "            raw_texts[caseid] = f.read().decode()\n",
    "        years[caseid] = int(year)\n",
    "        caseids.append(caseid)\n",
    "        counter_message(cnt, NUM_CASES, \"opened\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to `spacy` documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # requires:python -m spacy download en\n",
    "\n",
    "spacy_documents = {}\n",
    "for cnt, caseid in enumerate(caseids, start=1):\n",
    "    spacy_documents[caseid] = nlp(raw_texts[caseid])\n",
    "    counter_message(cnt, NUM_CASES, \"converted\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "normalized_docs = {}\n",
    "adjectives = set()\n",
    "\n",
    "for cnt, caseid in enumerate(caseids, start=1):\n",
    "    spacy_document = spacy_documents[caseid]\n",
    "    normalized_docs[caseid] = []\n",
    "    for sent in spacy_document.sents:\n",
    "        normalized_sent = []\n",
    "        for token in sent:\n",
    "            if (not token.is_punct) and (not token.is_space) and (token not in stoplist): \n",
    "                normalized_sent.append(token.lower_)\n",
    "            if token.pos_ == \"ADJ\":\n",
    "                adjectives.add(token.lower_)\n",
    "        if normalized_sent:\n",
    "            normalized_docs[caseid].append(normalized_sent)\n",
    "            \n",
    "    counter_message(cnt, NUM_CASES, \"normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as itt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sentences = list(itt.chain.from_iterable(normalized_docs.values()))\n",
    "\n",
    "# I expect the smaller window size to produce less qualitative, but more localized word embeddings, such that only\n",
    "# immediate neighbors are forced to be close together, whereas the long window size will have a more global\n",
    "# perspective, but word ordering will not matter as much.\n",
    "\n",
    "for window_size in (2, 16):\n",
    "    w2v = Word2Vec(\n",
    "        sentences,\n",
    "        min_count = 5,\n",
    "        window = window_size,\n",
    "        size = 50,\n",
    "        workers = 8,\n",
    "        iter = 25,\n",
    "        )\n",
    "    word_vectors = w2v.wv.vectors[:100]\n",
    "    projected = TSNE(n_components=2, perplexity=50, n_iter=350).fit_transform(word_vectors)\n",
    "    plt.figure()\n",
    "    plt.scatter(projected[:, 0], projected[:, 1], s=1)\n",
    "    for xy, word in zip(projected, w2v.wv.index2word[:100]):\n",
    "        plt.annotate(word, xy=xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep in (True, False):\n",
    "\n",
    "    sentences = list(itt.chain.from_iterable(v for k, v in normalized_docs.items() if is_republican[k]))\n",
    "    w2v = Word2Vec(\n",
    "        sentences,\n",
    "        min_count = 5,\n",
    "        window = 8,\n",
    "        size = 50,\n",
    "        workers = 8,\n",
    "        iter = 25,\n",
    "        )\n",
    "    print('---------------------')\n",
    "    print('REPUBLICAN' if rep else 'DEMOCRAT')\n",
    "    print('---------------------')\n",
    "    for word in ('abortion', 'gun', 'immigrant', 'drugs', 'church', 'race', 'tax'):\n",
    "        print('Most similar to {}'.format(word))\n",
    "        print([w for w, _ in w2v.most_similar(word, topn=100) if w in adjectives][:10])\n",
    "        print()\n",
    "\n",
    "# There are a number of differences in adjectives for each party, which reflect the political positions of the judges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
